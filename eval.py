import re
import json
import numpy as np
import argparse
import logging
# from smolagents import OpenAIServerModel
from openai import OpenAI
import os
import ipdb

class FutureXEvaluator:
    def __init__(self, api_key: str = None, base_url: str = None, file_name: str = None):
        """
        Args:
            api_key: OpenAI API key (å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä¸ä½¿ç”¨LLMçº æ­£)
            base_url: API base URL
        """
        self.use_llm = api_key is not None
        self.file_name = file_name
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        if self.use_llm:
            self.client = OpenAI(
                base_url="https://ark.cn-beijing.volces.com/api/v3",
                api_key=api_key,
            )
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # ç¡®ä¿evaluationç›®å½•å­˜åœ¨
        eval_dir = "evaluation"
        os.makedirs(eval_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
        if self.file_name:
            log_filename = self.file_name.replace('.json', '.log')
        else:
            log_filename = 'evaluation.log'
        
        log_path = os.path.join(eval_dir, log_filename)
        
        # é…ç½®logger
        self.logger = logging.getLogger(f'FutureXEvaluator_{id(self)}')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # åˆ›å»ºæ–‡ä»¶handler
        file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºformatter
        formatter = logging.Formatter('%(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def log_and_print(self, message):
        """åŒæ—¶è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶å’Œæ§åˆ¶å°"""
        self.logger.info(message)
    
    def _normalize_numerical_answer(self, pred: str, truth: str) -> tuple:
        """
        ä½¿ç”¨LLMå°†é¢„æµ‹å€¼å’ŒçœŸå®å€¼æ ‡å‡†åŒ–åˆ°åŒä¸€å•ä½ï¼ˆä»¥ground truthä¸ºå‡†ï¼‰
        Returns:
            (pred_value: float, truth_value: float)
        """
        if not self.use_llm:
            return (self._extract_number(pred), self._extract_number(truth))
        
        prompt = f""" 
Ground Truth: {truth}
Prediction: {pred}

Task: Convert the prediction to match the SAME format and unit as the ground truth.

Examples:
- Ground Truth: "10362 millions yuan", Prediction: "10362000000" 
  â†’ Ground Truth Value: 10362, Prediction Value: 10362
  
- Ground Truth: "4522.61", Prediction: "4.52261 thousand"
  â†’ Ground Truth Value: 4522.61, Prediction Value: 4522.61

- Ground Truth: "12740 million", Prediction: "12.74 billion"
  â†’ Ground Truth Value: 12740, Prediction Value: 12740

- Ground Truth: "15.5 billion", Prediction: "15500 million"
  â†’ Ground Truth Value: 15.5, Prediction Value: 15.5

IMPORTANT: 
1. Keep the SAME unit as ground truth (if GT is "millions yuan", convert prediction to millions yuan) ï¼
2. Extract only the numeric value, ignore unit text
3. If there is no numeric value in Prediction, set PREDICTION_VALUE to 0 directly
4. Return ONLY two numbers in this exact format:
   GROUND_TRUTH_VALUE: [number]
   PREDICTION_VALUE: [number]
5. No explanation, just the two numbers.

Now normalize:
"""
        
        try:
            completion = self.client.chat.completions.create(
                # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ IDï¼Œæ­¤å¤„å·²å¸®æ‚¨ä¿®æ”¹ä¸ºæ‚¨çš„æ¨ç†æ¥å…¥ç‚¹ ID
                temperature=0.2,
                model="deepseek-v3-1-terminus",
                messages=[
                    {"role": "system", "content": "You are a precise number normalization assistant."},
                    {"role": "user", "content": prompt},
                ],
            )
            response = completion.choices[0].message.content
            truth_match = re.search(r'GROUND_TRUTH_VALUE:\s*([\d.]+)', response)
            pred_match = re.search(r'PREDICTION_VALUE:\s*([\d.]+)', response)
            
            if truth_match and pred_match:
                truth_value = float(truth_match.group(1))
                pred_value = float(pred_match.group(1))
                return (pred_value, truth_value)
            else:
                # LLMæå–å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æå–
                self.log_and_print(f"Warning: LLM normalization failed, falling back. Response: {response}")
                return (self._extract_number(pred), self._extract_number(truth))
        except Exception as e:
            self.log_and_print(f"Error in LLM normalization: {e}")
            return (self._extract_number(pred), self._extract_number(truth))
    
    def _eval_numerical(self, pred: str, truth: str, std: float) -> float:
        """
        Level 3/4 æ•°å€¼å‹:
        score(Y, Å¶) = max(0, 1 - ((Y - Å¶) / Ïƒ(Y))Â²)
        """
        try:
            pred_value, true_value = self._normalize_numerical_answer(pred, truth)
            # print(f"  Pred Value: {pred_value}, True Value: {true_value}")
            if std == 0:
                return 1.0 if abs(pred_value - true_value) < 1e-6 else 0.0
            
            normalized_error = ((true_value - pred_value) / std) ** 2
            score = max(0.0, 1.0 - normalized_error)
            
            self.log_and_print(f"    LLM-Truth: {true_value}\n    LLM-Pred: {pred_value}\n    Std: {std}, Score: {score:.4f}")
            # with open(f"{self.file_name}_L4.txt", "a", encoding="utf-8") as f:
            #     f.write(f"Truth: {true_value} ï½œ Pred: {pred_value}\n")
            
            return score
        except Exception as e:
            self.log_and_print(f"Error in numerical evaluation: {e}")
            return 0.0
    
    def _eval_single_choice(self, pred: str, truth: str) -> float:
        """Level 1: score(Y, Å¶) = ğ•€(Y = Å¶)"""
        pred_options = self._extract_options(pred)
        pred_option = next(iter(pred_options)) if pred_options else ""
        truth_option = truth.strip()
        score = 1.0 if pred_option.upper() == truth_option.strip().upper() else 0.0
        self.log_and_print(f"  Truth: {truth_option}, Pred: {pred_option}, Score: {score:.4f}")
        return score
    
    def _eval_multi_choice(self, pred: str, truth: str) -> float:
        """Level 2: score(Y, Å¶) = F1-Score(Y, Å¶)"""
        pred_set = self._extract_options(pred)
        truth_set = self._extract_options(truth)
        
        if not pred_set and not truth_set:
            return 1.0
        
        intersection = len(pred_set & truth_set)
        precision = intersection / len(pred_set) if pred_set else 0
        recall = intersection / len(truth_set) if truth_set else 0
        
        if precision + recall == 0:
            score = 0.0
        else:
            score = 2 * precision * recall / (precision + recall)
        
        self.log_and_print(f"  Truth: {truth_set}, Pred: {pred_set}, F1: {score:.4f}")
        return score
    
    def _eval_ranking(self, pred: str, truth: str) -> float:
        """
        Level 3/4 æ’åºé¢˜:
        score = 1 if å®Œå…¨åŒ¹é…ï¼ˆé¡ºåºä¹Ÿå¯¹ï¼‰
        score = 0.8 Ã— |äº¤é›†| / k otherwise
        """
        pred_list = self._extract_list(pred)
        truth_list = self._extract_list(truth)
    
        if pred_list == truth_list:
            score = 1.0
        else:
            # éƒ¨åˆ†åŒ¹é…ï¼š0.8 Ã— (äº¤é›†å¤§å° / k)
            intersection = len(set(pred_list) & set(truth_list))
            k = len(truth_list)
            score = 0.8 * intersection / k if k > 0 else 0.0
        
        self.log_and_print(f"    Truth: {truth_list}")
        self.log_and_print(f"    Pred: {pred_list}")
        self.log_and_print(f"    Score: {score:.4f}")
        return score
    
    
    def _extract_options(self, text: str) -> set:
        """
        æå–é€‰é¡¹ï¼ˆæ”¯æŒå•é€‰ä¸å¤šé€‰ï¼‰ï¼š
        - ä¼˜å…ˆæå–å•ä¸ªå­—æ¯é€‰é¡¹ A/B/C...
        - è‹¥æœªåŒ¹é…åˆ°å­—æ¯é€‰é¡¹ä¸”å†…å®¹ä¸ºå•è¯ï¼ˆå¦‚ Yes/Noï¼‰ï¼Œè¿”å›è¯¥è¯ï¼ˆå¤§å†™ï¼‰ä½œä¸ºé›†åˆ
        """
        if not text:
            return set()

        boxed = re.search(r'\\boxed\{([^}]+)\}', text)
        content = boxed.group(1) if boxed else text

        # ä¼˜å…ˆæå–å­—æ¯é€‰é¡¹ï¼ˆA/B/C...ï¼‰
        letters = re.findall(r'\b([A-Z])\b', content.upper())
        if letters:
            return set(letters)

        # å…œåº•ï¼šå¦‚æœæ˜¯å•è¯ï¼ˆå¦‚ YES/NOï¼‰ï¼Œè¿”å›è¯¥è¯
        token = content.strip()
        if re.fullmatch(r'[A-Za-z]+', token):
            return {token.upper()}

        return set()
    
    def _extract_number(self, text: str) -> float:
        """æå–æ•°å€¼ï¼ˆfallbackæ–¹æ³•ï¼Œä¸è€ƒè™‘å•ä½ï¼‰"""
        numbers = re.findall(r'[\d.]+', text)
        if numbers:
            return float(numbers[0])
        raise ValueError("No number found")
    
    def _extract_list(self, text: str) -> list:
        """æå–æ’åºåˆ—è¡¨"""
      
        boxed = re.search(r'\\boxed\{([^}]+)\}', text)
        if not boxed:
            content = text
        else:
            content = boxed.group(1)
        
        content = re.sub(r'\d+(?:st|nd|rd|th)?[:ï¼š\.\s]+', '', content)
        
        items = re.split(r'[,ï¼Œã€;ï¼›\n]', content)
        items = [item.strip() for item in items if item.strip()]
        return items
    
    def evaluate_prediction(self, pred_answer: str, item: dict) -> float:
        """
        Args:
            pred_answer: æ¨¡å‹é¢„æµ‹çš„ç­”æ¡ˆï¼ˆåŒ…å«\\boxed{}æ ¼å¼ï¼‰
            item: ground_truthæ•°æ®é¡¹
        Returns:
            score: 0-1ä¹‹é—´çš„åˆ†æ•°
        """
        ground_truth = item['ground_truth']
        level = item['level']
        
        self.log_and_print(f"\n[Level {level}] ID: {item['id']}")
        self.log_and_print(f"  Ground Truth: {ground_truth}")
        self.log_and_print(f"  Prediction: {pred_answer}")
        
        if level == 1:  # Level 1: å•é€‰é¢˜
            return self._eval_single_choice(pred_answer, ground_truth)
        
        elif level == 2:  # Level 2: å¤šé€‰é¢˜
            return self._eval_multi_choice(pred_answer, ground_truth)
        
        elif level in [3, 4]:
            if item.get('Std') is not None:  # æ•°å€¼å‹é¢„æµ‹
                return self._eval_numerical(pred_answer, ground_truth, item['Std'])
            else:  
                return self._eval_ranking(pred_answer, ground_truth)
        
        return 0.0
    
    def evaluate_all(self, data: list) -> dict:
        """
        è¯„ä¼°æ‰€æœ‰é¢„æµ‹
        Args:
            data: JSONæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªitemåŒ…å« ground_truth å’Œ answer
                [
                    {
                        "id": "xxx",
                        "ground_truth": "...",
                        "answer": "\\boxed{...}",
                        "level": 1,
                        "Std": ...
                    },
                    ...
                ]
        Returns:
            {
                "overall_score": float,
                "level_scores": {1: float, 2: float, 3: float, 4: float},
                "level_counts": {1: int, 2: int, 3: int, 4: int}
            }
        """
        level_scores = {1: [], 2: [], 3: [], 4: []}
        
        for item in data:
            if 'answer' not in item:
                self.log_and_print(f"Warning: No answer found for ID {item['id']}")
                continue
            score = self.evaluate_prediction(item['answer'], item)
            level_scores[item['level']].append(score)
        
        weights = {1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4}
        overall = sum(
            np.mean(scores) * weights[level] 
            for level, scores in level_scores.items() 
            if scores
        )
        
        return {
            'overall_score': overall,
            'level_scores': {k: np.mean(v) if v else 0.0 for k, v in level_scores.items()},
            'level_counts': {k: len(v) for k, v in level_scores.items()}
        }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='FutureX è¯„ä¼°å·¥å…·')
    parser.add_argument('-f', '--file_name', default='20250915-random.json')
    
    args = parser.parse_args()
    
    evaluator = FutureXEvaluator(
        api_key="df7a9bcb-f50e-486d-9018-28eaf88aedfd",
        base_url="https://ark.cn-beijing.volces.com/api/v3/chat/completions",
        file_name=args.file_name
    )
    
    try:
        if not args.file_name.endswith('.json'):
            args.file_name += '.json'
        with open(os.path.join('prediction',args.file_name), 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        evaluator.log_and_print(f"æ­£åœ¨è¯„ä¼°æ–‡ä»¶: {args.file_name}")
        results = evaluator.evaluate_all(data)
        
        evaluator.log_and_print("\n" + "="*50)
        evaluator.log_and_print("è¯„ä¼°ç»“æœ:")
        evaluator.log_and_print(f"æ€»åˆ†: {results['overall_score']:.4f}")
        evaluator.log_and_print("\nå„çº§åˆ«å¾—åˆ†:")
        for level in [1, 2, 3, 4]:
            evaluator.log_and_print(f"  Level {level}: {results['level_scores'][level]:.4f} (å…± {results['level_counts'][level]} é¢˜)")
    
    except FileNotFoundError:
        evaluator.log_and_print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{args.file_name}'")
    except json.JSONDecodeError:
        evaluator.log_and_print(f"é”™è¯¯: æ–‡ä»¶ '{args.file_name}' ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
    except Exception as e:
        evaluator.log_and_print(f"é”™è¯¯: {e}")